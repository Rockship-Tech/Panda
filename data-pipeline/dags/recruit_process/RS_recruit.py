import os
import sys
import re
import tempfile
from airflow import DAG
from airflow.decorators import task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from botocore.exceptions import NoCredentialsError
from airflow.operators.python import PythonOperator
from pendulum import datetime
import logging
import pdfplumber
from PyPDF2 import PdfReader

from common.scripts.ITViecCVScraper import ITViecCVScraper
from common.scripts.pdf_converter import pdf_converter
from dotenv import load_dotenv

from airflow.models import Variable

# Load environment variables from .env file
load_dotenv()

AWS_CONN_ID = os.getenv("AWS_CONN_ID")
bucket_name = os.getenv("BUCKET_NAME")
CVs_path = os.getenv("CVs_path")
CVs_text_path = os.getenv("CVs_text_path")

default_args = {
    "retries": 0,
}


@task
def crawl_cvs_task():
    # Task 0: Crawl all CVs from itviec.com
    # ITviec.com login info
    # later save in airflow variable for more security
    login_url = os.getenv("ITVIEC_LOGIN_URL")
    applications_url = os.getenv("ITVIEC_APPLICATIONS_URL")
    username = os.getenv("ITVIEC_USERNAME")
    password = os.getenv("ITVIEC_PASSWORD")

    downloader = ITViecCVScraper()
    if downloader.login(login_url, username, password):
        downloader.download_all_cvs(applications_url, per_page=10)

    # Check if CVs have been downloaded
    if os.path.exists("CVs"):
        cv_files = os.listdir("CVs")
        cv_files = cv_files[:10]  # Select only the first 10 CVs for testing
        for filename in cv_files:
            if filename.endswith(".pdf"):
                pdf_file_path = os.path.join("CV", filename)
                logging.info(f"Downloaded {pdf_file_path}")


def download_from_s3(s3_file):
    # Task 1: Download the PDF file from S3 to local
    s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
    # local_pdf_path = f"/tmp/{os.path.basename(s3_file)}"
    file_name = s3_hook.download_file(
        key=s3_file,
        bucket_name=bucket_name,
        preserve_file_name=True,
        use_autogenerated_subdir=True,
    )
    logging.info(f"Downloaded {file_name}")
    return file_name


def convert_pdf_to_text_task(pdf_file_path):
    try:
        reader = PdfReader(pdf_file_path)

        text = ""
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text += page.extract_text()
        return text
    except:
        # Handle the error here (e.g., log the error, skip the file, etc.)
        logging.info(f"Error reading PDF file: {pdf_file_path}")
        return None


def upload_text_file(text, s3_file):
    # Task 3: Upload the text file to S3
    text_file_name = os.path.splitext(os.path.basename(s3_file))[0] + ".txt"
    logging.info(f"Text file name: {text_file_name}")
    local_text_path = f"/tmp/{text_file_name}"
    key_s3 = str("CVtext/") + text_file_name
    # Save the text to a local file
    with open(local_text_path, "w") as text_file:
        text_file.write(text)
    logging.info(f"Converted PDF to text: {local_text_path}")

    # Upload the text file to S3 using load_file
    s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
    s3_hook.load_file(
        filename=local_text_path, key=key_s3, bucket_name=bucket_name, replace=True
    )
    logging.info(
        f"Uploaded text file to S3: {local_text_path} , " + f"key: {key_s3} , "
    )
    return local_text_path


def remove_local_file(file_path):
    # Task: Remove the local PDF file
    os.remove(file_path)


def rename(file_path, cv_file):
    original_file_path = file_path
    logging.info(f"Original file path: {original_file_path}")
    # Get the directory of the original file
    directory = os.path.dirname(original_file_path)
    logging.info(f"Directory: {directory}")
    # Construct the new file path
    cv_file = str(cv_file)
    new_file_path = os.path.join(directory, cv_file[4:])
    logging.info(f"New file path: {new_file_path}")

    # Rename the file
    os.rename(original_file_path, new_file_path)
    return new_file_path


def retrieve_and_convert_pdfs_task():
    cv_files = S3Hook(aws_conn_id=AWS_CONN_ID).list_keys(
        bucket_name=bucket_name, prefix=CVs_path
    )
    converted_files = []
    for cv_file in cv_files:
        if cv_file.lower().endswith(".pdf"):
            file_path = download_from_s3(cv_file)
            logging.info(f"Prepare to convert: {file_path}")
            # pdf_file_path = rename(file_path, cv_file)

            text = convert_pdf_to_text_task(file_path)
            if text is None:
                continue
            # logging.info(f"Converted PDF to text: {text}")
            text_file_path = upload_text_file(text, cv_file)

            remove_local_file(file_path)  # Remove the local PDF file
            remove_local_file(text_file_path)  # Remove the local text file
            logging.info(f"Converted PDF to text: s3://{bucket_name}/{cv_file}")

            # converted_files.append(text_file_path)  # Collect the converted file paths

    return converted_files


with DAG(
    "cv_processing_dag",
    default_args=default_args,
    schedule_interval=None,
    start_date=datetime(2023, 1, 1),
    catchup=False,
    tags=["Rockship Recruitment Process"],
) as dag:
    # Task 0: Crawl all CVs from itviec.com
    crawl_cvs = crawl_cvs_task()

    # Task 1: Retrieve and convert PDFs to text
    cv_files = S3Hook(aws_conn_id=AWS_CONN_ID).list_keys(
        bucket_name=bucket_name, prefix=CVs_path
    )
    retrieve_and_convert_pdfs = PythonOperator(
        task_id="retrieve_and_convert_pdfs_task",
        python_callable=retrieve_and_convert_pdfs_task,
        dag=dag,
    )

    # Set task dependencies
    crawl_cvs >> retrieve_and_convert_pdfs
